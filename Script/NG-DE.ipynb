{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04723def-6455-469e-b737-d8544ccd34ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "dataset = pd.read_csv('Inputdata.csv')\n",
    "dataset.head()\n",
    "X = dataset.iloc[:,[3,4,5,6,7,8,9,10,11,12,13,14]].values\n",
    "y = dataset.iloc[:,[2]].values \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)  # 70% train, 30% remaining\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42)  # Split remaining 30% equally\n",
    "\n",
    "# Apply Standard Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "from ngboost import NGBoost\n",
    "from ngboost.distns import Normal   # you are using Normal\n",
    "from sklearn.metrics import mean_squared_error, brier_score_loss\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.calibration import calibration_curve\n",
    "from scipy.optimize import differential_evolution\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --------------------------\n",
    "# 1. DE optimisation (your code, just slightly tidied)\n",
    "# --------------------------\n",
    "def fitness_function(params, X_train, y_train, X_val, y_val):\n",
    "    n_estimators = int(params[0])\n",
    "    learning_rate = params[1]\n",
    "\n",
    "    model = NGBoost(\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=learning_rate,\n",
    "        Dist=Normal   # explicit, same as your original import\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    preds_val = model.predict(X_val)\n",
    "    mse = mean_squared_error(y_val, preds_val)\n",
    "    return mse  # DE minimises this\n",
    "\n",
    "lb = [50, 0.01]\n",
    "ub = [200, 0.2]\n",
    "\n",
    "result = differential_evolution(\n",
    "    fitness_function,\n",
    "    bounds=list(zip(lb, ub)),\n",
    "    args=(X_train, y_train, X_val, y_val),\n",
    "    strategy='best1bin',\n",
    "    popsize=40,\n",
    "    maxiter=100,\n",
    "    mutation=(0.5, 1),\n",
    "    recombination=0.7,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "best_solution = result.x\n",
    "best_fitness = result.fun\n",
    "\n",
    "print(f\"Optimized hyperparameters: n_estimators={int(best_solution[0])}, \"\n",
    "      f\"learning_rate={best_solution[1]:.4f}\")\n",
    "print(f\"Minimum validation MSE: {best_fitness:.4f}\")\n",
    "\n",
    "# Train best NGBoost on full training data\n",
    "best_regressor = NGBoost(\n",
    "    n_estimators=int(best_solution[0]),\n",
    "    learning_rate=best_solution[1],\n",
    "    Dist=Normal\n",
    ")\n",
    "best_regressor.fit(X_train, y_train)\n",
    "\n",
    "pred_test_uncal = best_regressor.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, pred_test_uncal)\n",
    "print(f\"Test MSE with best parameters: {test_mse:.4f}\")\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 2. Convert regression output to probabilities\n",
    "#    (for binary 0/1 target)\n",
    "# --------------------------\n",
    "# Predictions on val and test\n",
    "pred_val_uncal = best_regressor.predict(X_val)\n",
    "pred_test_uncal = best_regressor.predict(X_test)\n",
    "\n",
    "# Interpret as \"scores\" and clip to [0,1]\n",
    "proba_val_uncal = np.clip(pred_val_uncal, 0.0, 1.0)\n",
    "proba_test_uncal = np.clip(pred_test_uncal, 0.0, 1.0)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 3. Isotonic calibration on validation set\n",
    "# --------------------------\n",
    "iso = IsotonicRegression(out_of_bounds='clip')\n",
    "iso.fit(proba_val_uncal, y_val)\n",
    "\n",
    "# Calibrated probabilities on test set\n",
    "proba_test_cal = iso.predict(proba_test_uncal)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 4. Reliability curves (before vs after, with quantile bins)\n",
    "# --------------------------\n",
    "prob_true_uncal, prob_pred_uncal = calibration_curve(\n",
    "    y_test, proba_test_uncal, n_bins=15, strategy='quantile'\n",
    ")\n",
    "\n",
    "prob_true_cal, prob_pred_cal = calibration_curve(\n",
    "    y_test, proba_test_cal, n_bins=15, strategy='quantile'\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(prob_pred_uncal, prob_true_uncal, 'o-', label='Uncalibrated NGBoost (DE)')\n",
    "plt.plot(prob_pred_cal, prob_true_cal, 's-', label='Calibrated NGBoost (Isotonic)')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Perfect calibration')\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel(\"Predicted probability\")\n",
    "plt.ylabel(\"Observed frequency\")\n",
    "plt.title(\"Reliability diagram - NGBoost (DE-tuned)\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 5. Calibration metrics: Brier, ECE, sharpness\n",
    "# --------------------------\n",
    "def expected_calibration_error(y_true, y_prob, n_bins=15):\n",
    "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    bin_ids = np.digitize(y_prob, bins) - 1\n",
    "    ece = 0.0\n",
    "    n = len(y_true)\n",
    "    for b in range(n_bins):\n",
    "        mask = bin_ids == b\n",
    "        if np.any(mask):\n",
    "            prob_mean = y_prob[mask].mean()\n",
    "            freq_mean = y_true[mask].mean()\n",
    "            ece += (mask.sum() / n) * abs(prob_mean - freq_mean)\n",
    "    return ece\n",
    "\n",
    "brier_uncal = brier_score_loss(y_test, proba_test_uncal)\n",
    "brier_cal   = brier_score_loss(y_test, proba_test_cal)\n",
    "\n",
    "ece_uncal = expected_calibration_error(y_test, proba_test_uncal)\n",
    "ece_cal   = expected_calibration_error(y_test, proba_test_cal)\n",
    "\n",
    "sharp_uncal = np.var(proba_test_uncal)\n",
    "sharp_cal   = np.var(proba_test_cal)\n",
    "\n",
    "print(\"=== Calibration metrics (NGBoost-DE) ===\")\n",
    "print(f\"Uncalibrated - Brier: {brier_uncal:.4f}, ECE: {ece_uncal:.4f}, Sharpness: {sharp_uncal:.4f}\")\n",
    "print(f\"Calibrated   - Brier: {brier_cal:.4f}, ECE: {ece_cal:.4f}, Sharpness: {sharp_cal:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
